{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion Detection Using VGG16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rebubU5ZboSGHpMnhCQg4MyKVBz920t4",
      "authorship_tag": "ABX9TyPA/1Yj/p/FGffsZ6jpQcEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suchitra-V31/Deep_Learning_Projects/blob/main/Emotion_Detection_Using_VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8gNTRQa0SFu"
      },
      "source": [
        "**Emotion Detection Using VGG16**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGoHvtsjOBAL"
      },
      "source": [
        "In this notebook, we are going to handle the emotion dataset and make our model detect the emotion correctly.We take 7 emotions and train our model using transfer learning method - VGG16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eoo7Wxbt0R25"
      },
      "source": [
        "**Transfer Learning** is an approach where we use one model trained on a machine learning task and reuse it as a starting point for a different job."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgJ_hYp40Rqz"
      },
      "source": [
        "**VGG16** is a convolutional neural network trained on a subset of the ImageNet dataset, a collection of over 14 million images belonging to 22,000 categories\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuR7WSexOS_R"
      },
      "source": [
        "Let us first import all the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWGu2EXUumkW"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhtRNdnzxspj"
      },
      "source": [
        "from keras.layers import Input,Dense,Flatten,Lambda\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16  import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qOAehm6y8Sm"
      },
      "source": [
        "from glob import glob\n",
        "#In Python, the glob module is used to retrieve files/pathnames matching a specified pattern. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipdkh6T_zOXS"
      },
      "source": [
        "img_size=[224,224] #vgg16 has image size with 224\n",
        "img_channel=[3]\n",
        "img_shape=[224,224,3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnyk9k571Ugq"
      },
      "source": [
        "train_path='/content/drive/MyDrive/Datasets/Emotion train/emotion test'\n",
        "test_path='/content/drive/MyDrive/Datasets/Emotion recognition/Facial Expression -test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9XQrnXl1gZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec91cc0-be4b-4718-8275-a58a3b62df05"
      },
      "source": [
        "vgg=VGG16(include_top=False,weights='imagenet',input_shape=img_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_l6DxcP2PnH"
      },
      "source": [
        "We should not train any existing weights present in vgg16 since they are already trained and may cause overfitting of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFpVKWt71v3r"
      },
      "source": [
        "for layer in vgg.layers: \n",
        "  layer.trainable=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h9m8uoN1-ZS"
      },
      "source": [
        "folders=glob('/content/drive/MyDrive/Datasets/Emotion train/emotion test/*')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr7X0zcL3Vaq"
      },
      "source": [
        "The outputted feature stack will be 3-Dimensional, and for it to be used for prediction by other machine learning classifiers, it will need to be flattened."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwJ6S2vQ2pr9"
      },
      "source": [
        "x=Flatten()(vgg.output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41JXc_bC2peD"
      },
      "source": [
        "prediction=Dense(len(folders),activation='softmax')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2NnnTqV4ZrJ"
      },
      "source": [
        "**Create Your Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiuzRkjm2pSF"
      },
      "source": [
        "model=Model(inputs=vgg.input,outputs=prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoEx7Vvq2pBS",
        "outputId": "3289e1f6-5285-4c73-e65a-abf83ca7ae93"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 7)                 175623    \n",
            "=================================================================\n",
            "Total params: 14,890,311\n",
            "Trainable params: 175,623\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybGPxXCN5Dec"
      },
      "source": [
        "**Compile our Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGcVi2BD415r"
      },
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJC56TDq41yB"
      },
      "source": [
        "train_datagen=ImageDataGenerator(rescale=1./255,shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htg-Ucr25h9v"
      },
      "source": [
        "test_datagen=ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6q5QPUP6FVT",
        "outputId": "a75a688c-aeb3-4a1f-90d9-3183be9d9bde"
      },
      "source": [
        "training_set=train_datagen.flow_from_directory('/content/drive/MyDrive/Datasets/Emotion train/emotion test',target_size=(224,224),batch_size=32,class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6179 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKYpRalc41Yc",
        "outputId": "3e76ec94-246a-40c4-bfe9-ae776e5de09d"
      },
      "source": [
        "testing_set=test_datagen.flow_from_directory('/content/drive/MyDrive/Datasets/Emotion recognition/Facial Expression -test',target_size=(224,224),batch_size=32,class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5678 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-xsX6RmFmZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d02ca4-a1b5-4b5b-e2db-2b9453763642"
      },
      "source": [
        "result=model.fit_generator(training_set,validation_data=testing_set,epochs=10,steps_per_epoch=len(training_set),validation_steps=len(testing_set))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "194/194 [==============================] - 2390s 12s/step - loss: 1.8257 - accuracy: 0.3187 - val_loss: 1.9694 - val_accuracy: 0.3764\n",
            "Epoch 2/10\n",
            "194/194 [==============================] - 133s 687ms/step - loss: 1.5805 - accuracy: 0.4135 - val_loss: 1.6296 - val_accuracy: 0.4081\n",
            "Epoch 3/10\n",
            "194/194 [==============================] - 132s 682ms/step - loss: 1.5036 - accuracy: 0.4472 - val_loss: 1.5537 - val_accuracy: 0.4074\n",
            "Epoch 4/10\n",
            "194/194 [==============================] - 130s 672ms/step - loss: 1.3425 - accuracy: 0.4989 - val_loss: 1.7395 - val_accuracy: 0.4192\n",
            "Epoch 5/10\n",
            "194/194 [==============================] - 132s 682ms/step - loss: 1.3008 - accuracy: 0.5125 - val_loss: 1.8024 - val_accuracy: 0.4054\n",
            "Epoch 6/10\n",
            "194/194 [==============================] - 130s 672ms/step - loss: 1.2502 - accuracy: 0.5302 - val_loss: 1.9168 - val_accuracy: 0.3704\n",
            "Epoch 7/10\n",
            "194/194 [==============================] - 130s 673ms/step - loss: 1.2193 - accuracy: 0.5456 - val_loss: 1.6101 - val_accuracy: 0.4320\n",
            "Epoch 8/10\n",
            "194/194 [==============================] - 131s 676ms/step - loss: 1.2025 - accuracy: 0.5481 - val_loss: 1.8176 - val_accuracy: 0.4218\n",
            "Epoch 9/10\n",
            "194/194 [==============================] - 132s 683ms/step - loss: 1.1880 - accuracy: 0.5637 - val_loss: 1.5049 - val_accuracy: 0.4651\n",
            "Epoch 10/10\n",
            "194/194 [==============================] - 133s 685ms/step - loss: 1.0960 - accuracy: 0.5959 - val_loss: 1.8175 - val_accuracy: 0.4049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EullRV7LlUUt"
      },
      "source": [
        "We could see that our model has performed with 60% accuracy."
      ]
    }
  ]
}